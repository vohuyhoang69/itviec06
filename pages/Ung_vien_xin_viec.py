
import streamlit as st
import pandas as pd
import pickle
import re
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from googletrans import Translator
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from bertopic import BERTopic

# ===== Sidebar th√¥ng tin c√° nh√¢n =====
st.sidebar.markdown("""
    <style>
    .sidebar-info {
        font-size: 20px;
    }
    </style>
""", unsafe_allow_html=True)

st.sidebar.markdown("""
<div class="sidebar-info">
üë§ H·ªç v√† t√™n: <b>V√µ Huy Ho√†ng</b><br>
üìß Email: <b>2356210012@hcmussh.edu.vn</b>
</div>
""", unsafe_allow_html=True)

# ===== Ti√™u ƒë·ªÅ ch√≠nh =====
st.title("Trang d√†nh cho Ng∆∞·ªùi xin vi·ªác")
st.write("Tham kh·∫£o danh s√°ch c√¥ng ty ph√π h·ª£p v√† ƒë∆∞·ª£c Recommend cao.")

# ===== Load d·ªØ li·ªáu c√¥ng ty =====
df = pd.read_excel("Overview_Companies.xlsx")
df['Full_Description'] = (
    df['Company Name'].astype(str) + ' ' +
    df['Company Type'].astype(str) + ' ' +
    df['Company industry'].astype(str) + ' ' +
    df['Company size'].astype(str) + ' ' +
    df['Country'].astype(str) + ' ' +
    df['Working days'].astype(str) + ' ' +
    df['Overtime Policy'].astype(str) + ' ' +
    df['Company overview'].astype(str) + ' ' +
    df['Our key skills'].astype(str) + ' ' +
    df["Why you will love working here"].astype(str)
)

# ===== Load embedding ƒë√£ t√≠nh s·∫µn (phi√™n b·∫£n m·ªü r·ªông) =====
with open("bert_embeddings_enriched_stopword.pkl", "rb") as f:
    embeddings = pickle.load(f)

# ===== Kh·ªüi t·∫°o model BERT v√† d·ªãch =====
model = SentenceTransformer('distiluse-base-multilingual-cased-v1')
translator = Translator()

# ===== H√†m l√†m s·∫°ch d·ªØ li·ªáu ng∆∞·ªùi d√πng =====
def clean_user_input(text):
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)
    words = text.split()
    words = [word for word in words if word not in ENGLISH_STOP_WORDS]
    return ' '.join(words)

# ===== Giao di·ªán t√¨m ki·∫øm =====
st.title("üéØ T√¨m C√¥ng ty Ph√π h·ª£p v·ªõi Mong mu·ªën c·ªßa B·∫°n")

query_vi = st.text_input("Nh·∫≠p t·ª´ kh√≥a m√¥ t·∫£ mong mu·ªën (b·∫±ng ti·∫øng Vi·ªát ho·∫∑c ti·∫øng Anh)")

if query_vi:
    try:
        query_en = translator.translate(query_vi, src='vi', dest='en').text
    except:
        query_en = query_vi

    st.write(f"üîÑ T·ª´ kh√≥a sau khi d·ªãch (n·∫øu c·∫ßn): **{query_en}**")

    cleaned_query = clean_user_input(query_en)

    if len(cleaned_query.split()) < 0:
        st.warning("Vui l√≤ng nh·∫≠p m√¥ t·∫£ c·ª• th·ªÉ h∆°n (t·ªëi thi·ªÉu 3 t·ª´ c√≥ nghƒ©a).")
    else:
        query_vec = model.encode([cleaned_query])
        sim_scores = cosine_similarity(query_vec, embeddings).flatten()
        top_indices = sim_scores.argsort()[-5:][::-1]

        high_threshold = 0.3
        low_threshold = 0.1

        if sim_scores[top_indices[0]] >= high_threshold:
            st.success("‚úÖ T√¨m th·∫•y nh·ªØng c√¥ng ty th·ª±c s·ª± ph√π h·ª£p nh·∫•t:")
        elif sim_scores[top_indices[0]] >= low_threshold:
            st.info("‚ÑπÔ∏è Kh√¥ng c√≥ c√¥ng ty ho√†n to√†n kh·ªõp, nh∆∞ng d∆∞·ªõi ƒë√¢y l√† nh·ªØng c√¥ng ty g·∫ßn nh·∫•t v·ªõi mong mu·ªën c·ªßa b·∫°n:")
        else:
            st.warning("‚ö†Ô∏è Kh√¥ng c√≥ c√¥ng ty n√†o th·ª±c s·ª± ph√π h·ª£p v·ªõi t·ª´ kh√≥a b·∫°n nh·∫≠p.")
            st.stop()

        sorted_indices = sim_scores.argsort()[::-1]
        top_n = min(3, len(df))
        cols = st.columns(top_n)

        if "selected_idx" not in st.session_state:
            st.session_state.selected_idx = None

        for col, idx in zip(cols, sorted_indices[:top_n]):
            score = sim_scores[idx]
            company_name = df.iloc[idx]['Company Name']
            description = df.iloc[idx]['Full_Description']

            with col:
                with st.container():
                    st.markdown(f"### üè¢ {company_name}")
                    st.caption(f"Similarity: {score:.2f}")

                    if st.button(f"Xem th√¥ng tin m√¥ t·∫£", key=f"btn_{idx}"):
                        st.session_state.selected_idx = idx

                    href = df.iloc[idx].get('Href', '')
                    if href and href != 'nan':
                        st.markdown(f"[üîó Xem chi ti·∫øt tr√™n ITViec]({href})", unsafe_allow_html=True)

        if st.session_state.selected_idx is not None:
            idx = st.session_state.selected_idx
            st.write("---")
            with st.expander("üìù Th√¥ng tin chi ti·∫øt v√† ph√¢n t√≠ch th·ª±c t·∫ø", expanded=True):
                st.write(f"### üè¢ Th√¥ng tin chi ti·∫øt c·ªßa {df.iloc[idx]['Company Name']}:")
                st.write(f"**Lo·∫°i h√¨nh:** {df.iloc[idx]['Company Type']}")
                st.write(f"**Ng√†nh ngh·ªÅ:** {df.iloc[idx]['Company industry']}")
                st.write(f"**Quy m√¥:** {df.iloc[idx]['Company size']}")
                st.write(f"**T·ªïng quan c√¥ng ty:** {df.iloc[idx]['Company overview']}")
                st.write(f"**Qu·ªëc gia:** {df.iloc[idx]['Country']}")
                st.write(f"**Ng√†y l√†m vi·ªác:** {df.iloc[idx]['Working days']}")
                st.write(f"**Ch√≠nh s√°ch OT:** {df.iloc[idx]['Overtime Policy']}")
                st.write(f"**K·ªπ nƒÉng n·ªïi b·∫≠t:** {df.iloc[idx]['Our key skills']}")
                st.write(f"**V√¨ sao n√™n l√†m t·∫°i ƒë√¢y:** {df.iloc[idx]['Why you will love working here']}")
                st.write(f"**ƒê·ªãa ƒëi·ªÉm:** {df.iloc[idx]['Location']}")

                # H·ªèi th√™m ng∆∞·ªùi d√πng c√≥ mu·ªën xem ph√¢n t√≠ch review kh√¥ng
                if st.checkbox("T√¥i mu·ªën xem ph√¢n t√≠ch t·ª´ Review th·ª±c t·∫ø"):
                 # Load d·ªØ li·ªáu review
                    df_reviews = pd.read_excel("Reviews.xlsx")

                 # X·ª≠ l√Ω ng√†y th√°ng v√† t·∫°o c·ªôt NƒÉm
                    df_reviews['Cmt_day'] = pd.to_datetime(df_reviews['Cmt_day'], format='%B %Y', errors='coerce')
                    df_reviews['NƒÉm'] = df_reviews['Cmt_day'].dt.year


                    topic_model = BERTopic.load("bertopic_model")

                # Gh√©p n·ªôi dung review
                    df_reviews['Full_Review'] = (
                    df_reviews['Title'].astype(str) + ' ' +
                    df_reviews['What I liked'].astype(str) + ' ' +
                    df_reviews['Suggestions for improvement'].astype(str)
                    )

                # L·ªçc nh·ªØng d√≤ng c√≥ nƒÉm h·ª£p l·ªá
                    df_reviews = df_reviews.dropna(subset=['NƒÉm'])
                    df_reviews['NƒÉm'] = df_reviews['NƒÉm'].astype(int)

                # L·ªçc review c·ªßa c√¥ng ty hi·ªán t·∫°i
                    company_id = df.iloc[idx]['id']
                    df_company = df_reviews[df_reviews['id'] == company_id]

                    if df_company.empty:
                        st.info("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu review cho c√¥ng ty n√†y.")
                    else:
                        st.write("### üìä T·ªïng h·ª£p nhanh t·ª´ review c·ªßa nh√¢n vi√™n:")

                # L√†m s·∫°ch d·ªØ li·ªáu ƒë·ªÉ d·ª± ƒëo√°n
                    df_company['Full_Review_clean'] = df_company['Full_Review'].astype(str).apply(clean_user_input)

                # Load m√¥ h√¨nh ph√¢n lo·∫°i Recommend
                    with open("combo_model.pkl", "rb") as f:
                        combo = pickle.load(f)

                    vectorizer = combo["vectorizer"]
                    model = combo["model"]


                    X = vectorizer.transform(df_company['Full_Review_clean'])
                    preds = model.predict(X)
                    total = len(preds)
                    recommend_count = sum(preds)
                    percent = recommend_count / total * 100
                    score = round(percent * 5 / 100, 1)

                    st.write(f"- T·ªïng s·ªë review: **{total}**")
                    st.write(f"- T·ª∑ l·ªá Recommend: **{percent:.2f}%**")
                    st.write(f"- ƒêi·ªÉm t·ªïng h·ª£p: ‚≠êÔ∏è **{score}/5**")

                    if percent >= 60:
                        st.success("‚úÖ ƒê√°nh gi√° t·ªïng quan: C√≥ th·ªÉ c√¢n nh·∫Øc l√†m vi·ªác t·∫°i ƒë√¢y.")
                    elif percent <= 30:
                        st.error("‚ö†Ô∏è ƒê√°nh gi√° t·ªïng quan: Nhi·ªÅu review kh√¥ng khuy·∫øn ngh·ªã, c·∫ßn th·∫≠n tr·ªçng.")
                    else:
                        st.info("‚ÑπÔ∏è ƒê√°nh gi√° t·ªïng quan: √ù ki·∫øn tr√°i chi·ªÅu, n√™n t√¨m hi·ªÉu th√™m.")

                    st.divider()

                # Ph√¢n t√≠ch ch·ªß ƒë·ªÅ t·ª´ng nƒÉm
                    years = sorted(df_company['NƒÉm'].unique(), reverse=True)[:3]
                    years = sorted(years)

                    for year in years:
                        st.write(f"## üî• Top 5 ch·ªß ƒë·ªÅ ƒë∆∞·ª£c b√†n lu·∫≠n nhi·ªÅu nh·∫•t nƒÉm {year} c·ªßa c√¥ng ty **{df.iloc[idx]['Company Name']}**")

                        df_year = df_company[df_company['NƒÉm'] == year]
                        topics, _ = topic_model.transform(df_year['Full_Review'].tolist())
                        df_year['Topic'] = topics
                        df_year = df_year[df_year['Topic'] != -1]

                        if df_year.empty:
                            st.warning("Kh√¥ng c√≥ d·ªØ li·ªáu ch·ªß ƒë·ªÅ r√µ r√†ng cho nƒÉm n√†y.")
                        else:
                            top_topic_counts = df_year['Topic'].value_counts().head(5).reset_index()
                            top_topic_counts.columns = ['Topic', 'S·ªë l∆∞·ª£ng']

                            def extract_keywords(topic_id):
                                keywords = topic_model.get_topic(topic_id)
                                if isinstance(keywords, list):
                                    return " ".join([word for word, _ in keywords[:20]])
                                return ""

                            top_topic_counts['T·ª´ kh√≥a cho Wordcloud'] = top_topic_counts['Topic'].apply(extract_keywords)

                            for _, row in top_topic_counts.iterrows():
                                st.write(f"### üå•Ô∏è Ch·ªß ƒë·ªÅ {row['Topic']} - {row['S·ªë l∆∞·ª£ng']} l∆∞·ª£t nh·∫Øc ƒë·∫øn")

                                wordcloud = WordCloud(width=400, height=200, background_color="white").generate(row['T·ª´ kh√≥a cho Wordcloud'])

                                fig, ax = plt.subplots()
                                ax.imshow(wordcloud, interpolation='bilinear')
                                ax.axis("off")
                                st.pyplot(fig)

                    st.divider()



